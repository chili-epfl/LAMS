{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/foivos/anaconda3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from functools import reduce\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_from_csv(filename):\n",
    "    with open(filename, mode='r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        csv_dict = {rows[0]:rows[1] for rows in reader}\n",
    "        return csv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db(user, password, host, port, database):\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            user = user, password = password, host = host, port = port, database = database)\n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        print(\"Error: \", error)\n",
    "        \n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(connection, query_string):\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query_string)\n",
    "        records = cursor.fetchall()\n",
    "        print('Records fetched: ', len(records))\n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        print(\"Error: \", error)\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_paths(all_sequences_path):\n",
    "    open(all_sequences_path, 'w')\n",
    "    grouped_subsequences = subsequences.groupby('sequence_id')\n",
    "\n",
    "    for sequence_id in tqdm(grouped_subsequences.sequence_id.unique()):\n",
    "        with open(all_sequences_path, 'a') as f:\n",
    "            paths_list = []\n",
    "            lesson_subseqs = grouped_subsequences.get_group(sequence_id[0])\n",
    "            \n",
    "            # get the main sequence of the learning desing\n",
    "            main_seq = lesson_subseqs.loc[lesson_subseqs['main']==True]['activities']#values[0]\n",
    "            assert len(main_seq) == 1\n",
    "            main_seq = main_seq.values[0]\n",
    "            \n",
    "            # get all the branches excluding floating activities\n",
    "            subseq_dict = {}\n",
    "            lesson_subseqs = lesson_subseqs[(lesson_subseqs['main']==False) & (lesson_subseqs['parent_id'])]\n",
    "            for complex_act in lesson_subseqs.parent_id.unique():\n",
    "                subseq_dict[int(complex_act)] = []\n",
    "            \n",
    "            # replace complex activities with the relative branches \n",
    "            for row in lesson_subseqs.itertuples():\n",
    "                subseq_dict[int(row.parent_id)].append([row.parent_id] + row.activities)\n",
    "            subseq_dict\n",
    "            combinations = list(itertools.product(*list(subseq_dict.values())))\n",
    "            paths_list = []\n",
    "            for comp in combinations:\n",
    "                main_seq_copy = main_seq[:]\n",
    "                for subpath in comp:\n",
    "                    if subpath[0] in main_seq_copy:\n",
    "                        position = main_seq_copy.index(subpath[0]) + 1\n",
    "                        main_seq_copy[position: position] = subpath[1:]\n",
    "                if main_seq_copy not in paths_list:\n",
    "                    paths_list.append(main_seq_copy)\n",
    "\n",
    "            for path in paths_list:\n",
    "                f.write(str(sequence_id[0]) + ',')\n",
    "                for activity in path:\n",
    "                    f.write(str(activity) + ' ')\n",
    "                f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database connection\n",
    "params_dict = dict_from_csv('connection_params.csv')\n",
    "connection = connect_to_db(*params_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  connection already closed\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'records' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dad1694c4037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# query the database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msequence_records\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"select * from subsequences;\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m activity_records = execute_query( \n\u001b[1;32m      4\u001b[0m     \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"SELECT id, (CASE WHEN tool::text IS NULL THEN type::text ELSE tool::text END) as type FROM activities;\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2fc9c1e84589>\u001b[0m in \u001b[0;36mexecute_query\u001b[0;34m(connection, query_string)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'records' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# query the database\n",
    "sequence_records = execute_query(connection, \"select * from subsequences;\")\n",
    "activity_records = execute_query( \n",
    "    connection,\n",
    "    \"SELECT id, (CASE WHEN tool::text IS NULL THEN type::text ELSE tool::text END) as type FROM activities;\"\n",
    ")\n",
    "if connection:\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequences = pd.DataFrame(sequence_records, columns=['id', 'sequence_id', 'parent_id', 'activities', 'main'])\n",
    "subsequences.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>activities</th>\n",
       "      <th>main</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 7, 6, 9, 10, 8]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[11, 12, 13]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sequence_id  parent_id                          activities  main\n",
       "id                                                                  \n",
       "0             0        NaN  [0, 1, 2, 3, 4, 5, 7, 6, 9, 10, 8]  True\n",
       "1             1        NaN                        [11, 12, 13]  True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsequences.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question and Answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multiple Choice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   type\n",
       "id                     \n",
       "0   Question and Answer\n",
       "1       Multiple Choice"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activities = pd.DataFrame(activity_records, columns=['id', 'type'])\n",
    "activities.set_index('id', inplace=True)\n",
    "activities.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11240 paths were loaded\n"
     ]
    }
   ],
   "source": [
    "paths_file = Path(\"data/paths.txt\")\n",
    "\n",
    "if paths_file.is_file():\n",
    "    with open(paths_file, 'r') as f:\n",
    "        all_sequences = f.readlines()\n",
    "else:\n",
    "    create_all_paths(paths_file)\n",
    "    with open(paths_file, 'r') as f:\n",
    "        all_sequences = f.readlines()\n",
    "\n",
    "print(len(all_sequences), 'paths were loaded')\n",
    "# TODO f not found locally, create and keep it in memory without storing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,0 1 2 3 4 5 7 6 9 10 8'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sequences = [re.sub('\\n', '', line.strip()) for line in all_sequences]\n",
    "all_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a1337c8842433c9713e33b69d6e037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11240), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11240"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences = list()\n",
    "for sequence in tqdm(all_sequences):\n",
    "    text_seq = []\n",
    "    lesson_id = sequence.split(',')[0]\n",
    "    text_seq.append(lesson_id)\n",
    "    num_seq = sequence.split(',')[1]\n",
    "    for num in num_seq.split():\n",
    "        text_seq.append(activities.loc[int(num)].type)\n",
    "    text_sequences.append(text_seq)\n",
    "len(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4270"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eliminate dublicates within the same learning desing\n",
    "text_sequences_set = set(tuple(seq) for seq in text_sequences)\n",
    "text_sequences_unique = [list(seq) for seq in text_sequences_set]\n",
    "text_sequences_unique = sorted(text_sequences_unique, key=lambda x: int(x[0]))\n",
    "len(text_sequences_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/foivos/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# create dictionary -> lesson id: lesson n-grams\n",
    "# max n-gram counter per lesson = 1 \n",
    "ngram_dict = dict()\n",
    "text_seq_dict = dict()\n",
    "for seq in text_sequences_unique:\n",
    "    lesson_id = int(seq[0])\n",
    "    if lesson_id not in ngram_dict:\n",
    "        ngram_dict[lesson_id] = set()\n",
    "        text_seq_dict[lesson_id] = list()\n",
    "    ngram_dict[lesson_id].update(list(ngrams(seq[1:], ngram_num)))\n",
    "    text_seq_dict[lesson_id].append(seq[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counts = Counter()\n",
    "for ngram in ngram_dict.values():\n",
    "    ngram_counts += Counter(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Noticeboard', 'Noticeboard', 'Noticeboard'), 359)\n",
      "(('Noticeboard', 'Noticeboard', 'Multiple Choice'), 207)\n",
      "(('Noticeboard', 'Noticeboard', 'Share Resources'), 198)\n",
      "(('Noticeboard', 'Multiple Choice', 'Noticeboard'), 186)\n",
      "(('Noticeboard', 'Question and Answer', 'Noticeboard'), 175)\n",
      "(('Noticeboard', 'Noticeboard', 'Question and Answer'), 162)\n",
      "(('Noticeboard', 'Share Resources', 'Question and Answer'), 144)\n",
      "(('Noticeboard', 'Share Resources', 'Noticeboard'), 142)\n",
      "(('Noticeboard', 'GROUPING_ACTIVITY_TYPE', 'Chat'), 139)\n",
      "(('Noticeboard', 'Noticeboard', 'GROUPING_ACTIVITY_TYPE'), 133)\n"
     ]
    }
   ],
   "source": [
    "common_ngrams = ngram_counts.most_common(10) \n",
    "print(*common_ngrams, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/foivos/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# for each common ngram store the lesson ids where they appear\n",
    "ngram_set_list = []\n",
    "for ngram_tuple in common_ngrams:\n",
    "    ngram_set = set() \n",
    "    for seq in text_sequences_unique:\n",
    "        if ngram_tuple[0] in list(ngrams(seq[1:],ngram_num)):\n",
    "            ngram_set.add(int(seq[0]))\n",
    "    ngram_set_list.append(ngram_set)\n",
    "    \n",
    "for idx, lessons_set in enumerate(ngram_set_list):\n",
    "    common_ngrams[idx] = common_ngrams[idx] + (sorted(list(lessons_set)),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Locate n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_index(tokenized_sentence, requested_ngram, ngram_num=ngram_num):\n",
    "    try:\n",
    "        return list(ngrams(tokenized_sentence, ngram_num)).index(requested_ngram)\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the position of ngram in every lesson\n",
    "# # if it is the last ngram, assigns -1\n",
    "# for ngram in [common_ngrams[9], ()]:\n",
    "#     query_ngram = ngram[0]\n",
    "#     print(query_ngram, '\\n')\n",
    "#     for lesson_id in ngram[2]:\n",
    "#         ngram_positions = np.empty(len(text_seq_dict[lesson_id]))\n",
    "#         for idx, seq in enumerate(text_seq_dict[lesson_id]):\n",
    "#             ngram_positions[idx] = ngram_index(seq, query_ngram)\n",
    "#         lesson_position = np.floor(ngram_positions[ngram_positions >= 0].mean())\n",
    "#         if lesson_position + ngram_num == len(seq):\n",
    "#             print(lesson_position)\n",
    "#             lesson_position = -1\n",
    "#         print(lesson_id, '---->' , int(lesson_position))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigate the groupings\n",
    "# whether they are followed by branchings or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_set = set()\n",
    "group_branching_set = set()\n",
    "for seq in text_sequences:\n",
    "    if \"GROUP_BRANCHING_ACTIVITY_TYPE\" in seq[1:]:\n",
    "        group_branching_set.add(seq[0])\n",
    "    if \"GROUPING_ACTIVITY_TYPE\" in seq[1:]:\n",
    "        group_set.add(seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUPING_ACTIVITY_TYPE : 999 GROUP_BRANCHING_ACTIVITY_TYPE : 301\n",
      "Difference:  set()\n"
     ]
    }
   ],
   "source": [
    "print(\"GROUPING_ACTIVITY_TYPE :\", len(group_set), \"GROUP_BRANCHING_ACTIVITY_TYPE :\", len(group_branching_set))\n",
    "print(\"Difference: \", group_branching_set - group_set)\n",
    "group_no_branching_list = sorted([int(x) for x in (group_set - group_branching_set)])\n",
    "group_list = sorted([int(x) for x in group_set])        \n",
    "group_branching_list = sorted([int(x) for x in group_branching_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which activities cause tool branches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 42, 47, 51, 67]\n"
     ]
    }
   ],
   "source": [
    "myset = set()\n",
    "for seq in text_sequences:\n",
    "    if \"TOOL_BRANCHING_ACTIVITY_TYPE\" in seq[1:]:\n",
    "        myset.add(seq[0])\n",
    "mylist = sorted([int(x) for x in myset])\n",
    "print(mylist[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['881', 'Image Gallery', 'eAdventure', 'TOOL_BRANCHING_ACTIVITY_TYPE', 'Image Gallery', 'eAdventure', 'Notebook']\n",
      "['881', 'Image Gallery', 'eAdventure', 'TOOL_BRANCHING_ACTIVITY_TYPE', 'Notebook']\n"
     ]
    }
   ],
   "source": [
    "for seq in text_sequences:\n",
    "    if '881' in seq:\n",
    "        print(seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
